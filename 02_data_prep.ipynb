{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f9d431-06de-48e6-9d38-716441a7b192",
   "metadata": {},
   "source": [
    "# MLOps for RSEs - 2. Data Preparation and Exploration\n",
    "\n",
    "The first part of a machine learning project is to producer a dataset that can be used to train and optimise one or more machine learning models. In many ways this part is the most familiar for RSEs as dataset are gathered and processed routinely for almost all research projects rather than being a particular requirement for ML projects. There are some challenges in creating a *good* dataset for use in an ML project. This notebook will look at what the requirements for the data pipeline are that RSEs will need to support and some tools and practices that can make this process easier. \n",
    "\n",
    "![Met Office Logo](https://www.metoffice.gov.uk/webfiles/1661941781161/images/icons/social-icons/default_card_315.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e3246-4f1a-4269-8c37-155758f0605f",
   "metadata": {},
   "source": [
    "## Building a data preparation pipeline\n",
    "\n",
    "Exactly what a data preparation pipeline will look like is of course very problem and domain dependant. Of all the parts of a machine learning project, this is the part that is most likely to include \"domain specific\" tools, that is tools that are specific to the particular area of research covered by the project, rather than generic data science or machine learning tools that will be used in most of the rest of an ML project. The key parts of a pipeline include:\n",
    "\n",
    "* Data Gathering - The first step is to get the data together that will be used to create the final output. The data might be coming from public datasets, generated by a simulation, gathered from observation (either by you or some one else), scraped from the web or many other possible locations. Many ML projects involve combining data from different sources. Some times the data are easy to use together, other times a lot of subsequent processing will be required for the data to be *analysis-ready*, or more specifically *machine learning-ready*.\n",
    "  * Distributed processing - In some cases you may like to do some of the subsequent processing steps in a distributed fashion wherever the data is located rather than first gathering the data in a singhle location, particular where the processing involves subsetting and reduction that result in a much smaller output than input. Generally data transfer is expensive, so a good practice is to *take the compute to the data*. For example if data is located on a particular server and you need to extract data for a particular temporal and spatial location, it would be prereable to do that on the server if possible and then only copy accross the data to be used to the central processing location for the project.\n",
    "* Cleaning - Removing bad data or infilling missing data. Homogenising representations of data e.g. names of categories.\n",
    "* Subsetting - Select the subset of interest for training your machine learning model.\n",
    "* Transformation - Change the representation of your data to be consistent across data sources e.g. representing geospatial data on the same projection and grid.\n",
    "* Calculation of derived fields - A feature that combines various fields from raw data may be a better predictor, so such fields are typically calculated as part of the data preparation.\n",
    "* Merging - Many machine learning projects bring together data from several sources which once the previous steps have been performed are merged into a single coherent dataset.\n",
    "* Storing - Once the dataset has been created, it needs to be stored somewhere for reuse, ideally accordinging to FAIR principles and somewhere access if efficient.\n",
    "* Cataloging and Documenting - Add your dataset to a catalog or other collection of datasets to enable to eassily find and (re)use it.\n",
    "* Exploration and Visualisation - Enable researchers to explore the dataset through visualiations and summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb68695-fd67-4157-ad92-2b2d172a0ed9",
   "metadata": {},
   "source": [
    "### Key Principles\n",
    "* *reusability* - Create data processing components that are well engineered and can be reused in multiple pipelines.\n",
    "* *scalability* - Be able to easily run processing workflows with whatever size dataset is required by the research problem.\n",
    "* *interactivity* -  Enable research to interact with and epxlore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbd0a9-b1a3-4529-bb6b-22e723fa4081",
   "metadata": {},
   "source": [
    "### Key Tasks for RSEs\n",
    "* Setting up scalable infrastructure for running pipeline on large data\n",
    "* Ensuring datasets created through pipeline are producible\n",
    "* Informing design to ensure processing components are reusable\n",
    "* Ensuring good management, documentation and testing of pipeline code\n",
    "* Setting up infrastructure for sharing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010305d0-0fca-4e81-b9c9-827835567ed7",
   "metadata": {},
   "source": [
    "### Key Terms\n",
    "* *FAIR* or *Findable, Accessible, Interoperable, Reusable* -  \n",
    "* *ARCO data* or *Analysis-ready, cloud-optimised data* - \n",
    "* pipeline - A series of tasks or operations, preferably based on resuable components, where the input of one step in the pipeline is the output of the previous step.\n",
    "* workflow - similar to a pipeline as a series of tasks, but there may be multiple consecutive or parallel pipelines in a workflow. (Definitions may differ!)\n",
    "* catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810862a-47bb-46c0-9ffb-910474a1049b",
   "metadata": {},
   "source": [
    "### Key Tools\n",
    "\n",
    "Many of the steps will be making use of the domain knowledge of researchers in project (e.g. ecologists, doctors, economists, pyschologists etc.) and will need to make use of domain specific tools to handle the particular sorts of data. There are also any generic components, particularly around infrastructure, that can support the use of the domain specific tools:\n",
    "* workflow management and scaling (dask, ray)\n",
    "* environment management (conda, docker)\n",
    "* visualisation (matplotlib, holoviz)\n",
    "* catalog (intake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324b2cb-21af-48a5-8477-163433918ff8",
   "metadata": {},
   "source": [
    "### Running this notebook\n",
    "This notebook should run from a conda environment created with the [requirements_data_prep.yml file](requirements_data_prep.yml). See the [readme file](https://github.com/informatics-lab/ukrse_2022_mlops_walkthrough/blob/main/README.md) for info on how to set up a conda environment for using this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b101ce-2ca2-413e-97b9-9987b41854f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example Problem -  Predicting impactful turbulent wind gusts\n",
    "\n",
    "The first dataset we will explore comes from a challenge by a Met Office Chief Operational Meteorologist. In certain locations around the world turbulent lee wave gusts called *rotors* can cause problems for aviation. These are localised topographically-driven turbulent wind gusts and where these occur at an airfiel make conditions siufficiently hazardous so as to cause operations to be suspended. Rotors therefore impact the aviation industry, in partticular where flights have to be diverted  for considerable distances from isolated airfields.  \n",
    "\n",
    "What is needed is a prediction of whether such rotors are likely in a given time window, so that flight can be postponed or cancelled rather than risking the flight and turning back wasting fuel. As these are small scale features, they are not explicitly resolved in coarse resolution global models. Human OpMets are able to look at this model data and \n",
    " determine whether rotors are likely, but this is error-prone and time consuming. Can we train a machine learning algorithm to act as a tool to help OpMets with this process and provide a second opinion. In particular, a machine learning solution can easily look at multiple model outputs (ensemble output, different forecast centres e.g. ECMWF).\n",
    "\n",
    "The target is a human-created dataset produced by Operational Meterologists at the airfield, as to whether a rotor occured in any given three hour period.\n",
    "\n",
    "![Lee Wave Rotors Diagram](rotors_diagram.png)\n",
    "\n",
    "[Met Office Youtube video explaining rotors](https://www.youtube.com/watch?v=jgSZG9SqN_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af214b8c-aff7-44bc-a933-42b75964cec8",
   "metadata": {},
   "source": [
    "## Example 1 - Exploring and preparing Wind Rotors data\n",
    "\n",
    "We will start. by exploring the wind rotors data set, and cleaning and prearing the data for machine learning as described above. First we will load in the \"raw\" dataset into a data frame. In this particular instance we won't use the standard library for dataframe *pandas*, instead wel will the dataframe implementation supplied dask. This support *lazy loading* natively, so instead of loading our dataset that may be too large to fit in memory, or may just require waiting a long time for it to load, it reads only the *metadata* which tells it enough to construct a stub data strcuture and plan the computations. Any subsequent calculation will similarly not actually be executed when called, but rather continue the *lazy execution* paradigm. This creates a *task graph* whivch represents the chain of tasks in our workflow. \n",
    "\n",
    "When we have constrcuted the graph representing the workflow, we can then trigger executiomn of all the steps. An additional advantage is that we can send the workflow to another, potentially shared remote, platform from our interactive compute (eg. local laptop, notebooks server) which may have more substantial resources (more memory, many CPUs, specialised processors like GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d81711-60f1-4996-b808-f24760cbc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import functools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa9221-d29c-4ee4-81d9-1304537d3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef7864-09de-4d14-8a0e-aa66b1cac6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8d33b-be41-4a20-b636-9395f6c3606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe\n",
    "import dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4e509-8033-4373-b78b-d4dfdb29a524",
   "metadata": {},
   "source": [
    "The data is available from zenodo. See the [readme file](README.md) file for details. You can download the data into the default location of `~/data/ukrse2022` or you can specify the location in the environment variable `${RSE22_ROOT_DATA_DIR}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73805561-5820-4b3b-b856-cb359b84ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rse_root_data_dir = pathlib.Path(os.environ['RSE22_ROOT_DATA_DIR'])\n",
    "    print('reading from environment variable')\n",
    "except KeyError as ke1:\n",
    "    rse_root_data_dir = pathlib.Path(os.environ['HOME'])  / 'data' / 'ukrse2022'\n",
    "    print('using default path')\n",
    "rse_root_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8688c7-7db6-4b4d-94bd-fd0c504e6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_zenodo=False\n",
    "zenodo_record_root = 'https://zenodo.org/record/7022648/files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63754eaa-add1-4269-9100-0abe550ada99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(f'{zenodo_record_root}') / '2021_met_office_aviation_rotors.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67317ad-7819-406e-ba53-e9cbbd0388c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_fname = '2021_met_office_aviation_rotors.csv'\n",
    "if load_from_zenodo:\n",
    "    rotors_path = f'{zenodo_record_root}/{rotors_fname}'\n",
    "else:\n",
    "    rotors_path = rse_root_data_dir / rotors_fname\n",
    "    print(rotors_path.is_file())\n",
    "rotors_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a0c6a8-de8b-4483-a3f4-b0decb2ba4fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up the cluster - Dask\n",
    "\n",
    "For this example we are using a simple local cluster. This is the only part of the code that would need to change to execute the subequent code on a large cluster or cloud based cluster (in theory at least, there are often addtional steps needed around python environments, which are getting easier as the tools mature).\n",
    "\n",
    "There are two steps in. this part, firstly initiating the cluster, then creating a client to access the cluster. In a complicated separate, the cluster set up step is likely to happen elsewhere through a command line interface, or together with a container  or  cloud orchestration software like [kubernetes](https://kubernetes.io/) or [docker swarm](https://docs.docker.com/engine/swarm/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bb929-93ee-4789-ab90-3fc276b6f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_cluster = dask.distributed.LocalCluster(n_workers=4)\n",
    "local_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb88a58-70a8-4f0d-af8e-cbbc40e0e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client = dask.distributed.Client(local_cluster)\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b696fa-949a-4129-82ee-d74bb684043f",
   "metadata": {},
   "source": [
    "### Prepare and execute data reading tasks\n",
    "\n",
    "Now that we have prepared the cluster for our tasks, we can set up workflow through a series lazy execution function calls. The code looks exactly the same as if the computation were to happen immediately, but instead if we look at the returned objects, we don't see the actual data objects, but *delayed* proxy objects which contain the data required for executing the workflow tasks when requested to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3954b1-5f68-4a2c-bff1-c65831d98a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df = dask.dataframe.read_csv(rotors_path)\n",
    "rotors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6082abb-89d4-43cf-a1ad-ae005cbdcffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d8bf1-b2ca-498e-a1a5-399dce6d382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x = rotors_df['rotors_present']\n",
    "except KeyError:\n",
    "    rot_df = rotors_df.compute()\n",
    "    rot_df = rot_df.rename(columns={'Rotors 1 is true' : 'rotors_present'})\n",
    "    rot_df['rotors_present'].fillna(value=0.0, inplace=True)\n",
    "    os.remove(rotors_path)\n",
    "    rot_df.to_csv(rotors_path)\n",
    "    rotors_df = dask.dataframe.read_csv(rotors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945b6c3-a60d-4c3c-b44b-97e50882a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature_name = 'rotors_present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56234e9-1062-40fc-893d-861ed205d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df  = rotors_df [(rotors_df ['wind_speed_obs'] >= 0.0) &\n",
    "                            (rotors_df ['air_temp_obs'] >= 0.0) &\n",
    "                            (rotors_df ['wind_direction_obs'] >= 0.0) &\n",
    "                            (rotors_df ['dewpoint_obs'] >= 0.0) \n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c94c1b-1d8a-45cf-b9c9-065e023bb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66406cce-2550-4c77-a725-41bffc03f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df ['DTG'] = dask.dataframe.to_datetime(rotors_df['DTG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b81d5-062e-475b-a44d-95fb760e891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df  = rotors_df .drop_duplicates(subset=['DTG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a89fc-1cbf-4ba2-89cd-b46e38f4c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df  = rotors_df [~rotors_df['DTG'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1c70b-b84b-4f99-b899-166475c5a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df_delayed = rotors_df\n",
    "rotors_df_delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d8924-f554-4db8-b6b3-e8bbef59a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rotors_df = rotors_df[rotors_df[target_feature_name] == False]\n",
    "rotors_present_df = rotors_df[rotors_df[target_feature_name] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772aff86-bddd-4a81-8339-1c003e3ffb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rotors_df.shape, rotors_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208ed34-33ad-4f01-a905-64887daefe6c",
   "metadata": {},
   "source": [
    "To get the data, we have to execute the task graph on the cluster. We should now see something happen on the dask dashbaord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f873d-8089-466b-bf13-ef5e07f31174",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rotors_df = no_rotors_df.compute()\n",
    "rotors_present_df = rotors_present_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf48ad8-c8b4-4c9b-a83f-030d7c615835",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rotors_df.shape, rotors_present_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d7c9f-1984-4f84-abee-dfa14a058573",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rotors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34399a0-cd53-425e-81b0-122e949106e4",
   "metadata": {},
   "source": [
    "Part of the data preparation process is often to create derived features that will be more useful as input for the machine learning algorithm. This is one way to include the knowledge of researchers into a machine learning pipeline through *feature engineering* which embed the known relationships and important values in the data in the way the dataset is presented for training. \n",
    "\n",
    "Here we want to present wind as two vectors in the u direction (east-west) and the v-direction (north-south), as our research meteorlogists know this is likely to be more useful in predicting these wind rotors events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123c72c-7eae-44c2-b909-0be1cfe5acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b350f-23fa-4a66-acd6-5c77311293d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.cos(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]\n",
    "\n",
    "def get_u_wind(wind_dir_name, wind_speed_name, row1):\n",
    "    return math.sin(math.radians(row1[wind_dir_name])) * row1[wind_speed_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049d9c1-6b31-46c6-968b-e1334468f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "u_feature_template = 'u_wind_{level_ix}'\n",
    "v_feature_template = 'v_wind_{level_ix}'\n",
    "u_wind_feature_names = []\n",
    "v_wind_features_names = []\n",
    "for wsn1, wdn1 in zip(wind_speed_feature_names, wind_direction_feature_names):\n",
    "    level_ix = int( wsn1.split('_')[1])\n",
    "    u_feature = u_feature_template.format(level_ix=level_ix)\n",
    "    u_wind_feature_names += [u_feature]\n",
    "    rotors_df[u_feature] = rotors_df.apply(functools.partial(get_u_wind, wdn1, wsn1), axis='columns', meta=(None, 'float64'))\n",
    "    v_feature = v_feature_template.format(level_ix=level_ix)\n",
    "    v_wind_features_names += [v_feature]\n",
    "    rotors_df[v_feature] = rotors_df.apply(functools.partial(get_v_wind, wdn1, wsn1), axis='columns', meta=(None, 'float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8866baa-3f46-4061-b5ac-ae6481bcd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df = rotors_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc5924-5c6b-4f05-95a1-dc4349908783",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df['time'] = rotors_df['DTG']\n",
    "rotors_df = rotors_df.drop(['DTG'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84623144-1844-4a33-97e1-a68904a03135",
   "metadata": {},
   "source": [
    "Our data is now cleaned up and ready to be saved our as a preprocessed, analysis ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd943b-472c-492a-b65f-87c6a62fde97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468910d-6454-41a9-bc27-c2ceb0c3720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_preprocessed_path = rse_root_data_dir / (pathlib.Path(rotors_fname).stem + '_preprocessed.csv')\n",
    "rotors_preprocessed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a566a-7597-4d9c-92c7-f87910b2611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df.to_csv(rotors_preprocessed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8f777-ca9f-4191-aab7-fd3cf76eec4e",
   "metadata": {},
   "source": [
    "### Create an interactive dashboard for data exploration\n",
    "Part of the initial process of a machine learning project is to explore the contents of the dataset, especially where it is unfamiliar to those researcvhers working with it. You may wish to get a sense of the values,s trcutures and relationships in the data. Thyere are a variety of ways to do this, through visualising the \"raw\" data directly, or summary statitics and derived metrics and indices which describe the data Often it will be helpful to be able to share both the results of the *exploratory data analysis* and access to exploratory code itself. This can be done without having to run code directly by quickly creating a *dashboard* use one of the many python, R, javascript or other langiuage tools to do this. This allows a range of experts in different areas to explore the data to inform subsequent data analysis and modelling choices use in the machine learning pipeline.\n",
    "\n",
    "Here we demonstrate the use of tools in the [holoviz ecosystem](https://holoviz.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd911ecd-bdd3-4f97-8717-b47e98cb662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel\n",
    "panel.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e01c6-9f01-4a54-a3d5-2914997c278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749428e9-7cba-4327-9b72-c0f25ac1d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars_dict = {\n",
    "    'Temperature': 'air_temp',\n",
    "    'Specific Humidity': 'sh',\n",
    "    'Wind Direction': 'winddir' ,\n",
    "    'Wind Speed': 'windspd' ,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66447a-244a-4686-8b10-eba2f0cd75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dict = {\n",
    "    'Temperature': 'air_temp_obs',\n",
    "    'Dew Point Temperature': 'dewpoint_obs',\n",
    "    'Wind Direction': 'wind_direction_obs' ,\n",
    "    'Wind Speed': 'wind_speed_obs' ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1859bbe-dfad-4674-a151-ce9e698d2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_histogram(plotting_df, rotors_feature_name, selected_var, subset):\n",
    "    fig1 = matplotlib.pyplot.figure(f'histogram of {selected_var}')\n",
    "    ax1 = fig1.add_subplot(1,1,1,title=f'histogram of {selected_var}')\n",
    "    if subset == 'all':\n",
    "        data_to_plot = plotting_df\n",
    "    elif subset == 'no_rotors':\n",
    "        data_to_plot = plotting_df[plotting_df[rotors_feature_name] == 0.0]\n",
    "    elif subset == 'rotors_present':\n",
    "        data_to_plot = plotting_df[plotting_df[rotors_feature_name] == 1.0]\n",
    "    _ = data_to_plot[selected_var].plot.hist(bins=20, ax=ax1)\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda48027-d3dc-4d9a-b295-401b4b6f14c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_var_select = panel.widgets.Select(\n",
    "    options=obs_dict,\n",
    "    name='selected_var',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b2288-a329-4c51-b0e6-647a9925751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_select = panel.widgets.Select(\n",
    "    options = ['all', 'no_rotors', 'rotors_present'],\n",
    "    name='subset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8f948-da65-49b1-9fb2-128e3a92880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plotter = panel.bind(\n",
    "    functools.partial(\n",
    "        do_histogram,\n",
    "        plotting_df=rotors_df,\n",
    "        rotors_feature_name=target_feature_name,\n",
    "        ),\n",
    "    selected_var=hist_var_select,\n",
    "    subset=subset_select,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a5dfa-16ee-47c0-9902-d260b95422f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_dash = panel.Column(\n",
    "    panel.widgets.StaticText(value='Rotors Data Exploration Dashboard'),\n",
    "    panel.Row(subset_select, hist_var_select),\n",
    "    hist_plotter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391151c3-3781-4824-9a43-5aa5dd6578ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_dash.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cf425-6592-4e16-bad7-1692dee2508e",
   "metadata": {},
   "source": [
    "In the above example, we see a very simple dashboard to visualise some statsitics about the data. This could be extended to any particular plots that are of interest to the researchers, which can easily be created in a standard script or notebook and then subsequently included in dashboard code. \n",
    "\n",
    "This code could also be put in it own script of course, or run run on a website directly from the notebook using the panel library from the command line:\n",
    "``\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3a8ae-5fea-43ab-8063-a97d9b2dd6ff",
   "metadata": {},
   "source": [
    "## Example 2 - Weather Types from ERA5 Reanalysis Dataset\n",
    "\n",
    "Our next example further explores processing at scale on a different sort of data often called *gridded data* in environmental science. This is essentially data that has a similar strcutre to image or more generally *raster* data. The difference here is that our grid may extend in n dimensions, such as latitude, longitude, height above the surface, time, ensemble realisation etc. Here we use the ERA5 reanlsysis dataset to create \"weather regime clusters\", that is find groups of air pressure patterns that are similar and are likely to be associated with similar sorts of weather. One could use experts to describe what they think such patterns would be, or one can use a data driven approach like clustering to find what patterns are actually present in the data and may reveal paterns that had not been previously considered.\n",
    "\n",
    "Further information on weather regimes:\n",
    "* https://www.metoffice.gov.uk/services/business-industry/energy/decider\n",
    "* https://www.metoffice.gov.uk/research/news/2016/new-weather-patterns-for-uk-and-europe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af61e77-6b71-4df8-aeeb-f345906e07a1",
   "metadata": {},
   "source": [
    "The particular challenges in this problem are again around the scale of the data requiring processing, but also around the use of doamin specifc libraries for processing. We will often need to use various domain specific tools and techniques in the data preparation part of the pipeline to prepare the data for use with more generic machine learning and data science tools. In this example we are using the Met Office python library [Iris](https://scitools-iris.readthedocs.io/en/stable/), which handles gridded meteorological (i.e. climate and weather) data and metadata and includes various meteorology specific functionality and handles specific data storage formats such as *NetCDF* and *Grib2*. Other research domains will have similar packages for handling their specific needs, which will similarly need to be supported as part of a machine learning projects in those research domains.\n",
    "\n",
    "Docs:\n",
    "* [Iris](https://scitools-iris.readthedocs.io/en/stable/)\n",
    "* [Xarray](https://docs.xarray.dev/en/stable/) (similar package) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bdb50-4e64-458c-b901-b26c1cdfc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.coord_categorisation\n",
    "import iris.quickplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f0ce6-0cf7-44bf-8865-27d26637fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy\n",
    "import cartopy.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cc8f6-2ec3-4791-8861-6ad4529bc8d9",
   "metadata": {},
   "source": [
    "### Accessing the raw data\n",
    "The full global ERA5 files can most easily be accessed through a [public bucket on s3](https://registry.opendata.aws/ecmwf-era5/)\n",
    "You can either follow the instructions to access it directly from python using the instrcutions on the above link, or you can download a local copy using the aws s3 command line tool as follows:\n",
    "\n",
    "`aws s3 cp --no-sign-request s3://era5-pds/2019/01/data/air_pressure_at_mean_sea_level.nc ~/data/ukrse2022/`\n",
    "\n",
    "The data files are for one month at a time, so you would need to do this for all months from January 2017 to December 2019 to execute the code below. The total size for that data is 35GB, so you may wish to rather just used the preprepared subset available through zenodo as described in the readme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee36a9-356d-4cbe-b3a7-b411aab36835",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    era5_data_dir = pathlib.Path(os.environ['RSE22_ERA5_DATA_DIR'])\n",
    "    print('reading from environment variable')\n",
    "except KeyError as ke1:\n",
    "    era5_data_dir = pathlib.Path(os.environ['HOME'])  / 'data' / 'ukrse2022'\n",
    "    print('using default path')\n",
    "era5_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bce62b-b473-4acb-9f16-93261437efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_global_paths = sorted([p1 for p1 in era5_data_dir.iterdir() if '.nc' in str(p1) and 'air_pressure' in str(p1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18486bcf-c1a7-4d49-be63-00950ad7c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_na_bounds = {'latitude': (40,65), 'longitude': (-10,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a154744-c15b-496f-830d-c46c06a83195",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_global_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30e6ef-f6ad-4e60-b975-9b98911e7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_mslp_uk_subset_fname = 'era5_mslp_UK_2017_2020.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee8066-ed13-400f-be36-04b0ec69d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b758b9-bdc7-42e5-8abc-0786a808f215",
   "metadata": {},
   "source": [
    "In the code inital exract code below, we load all the downloaded data files, tidy up some of the metadata using the `equalise_attributes` function, then join the data from each month into a single *cube*, which is what iris calls a multidimensional gridded dataset describing a single physical phenomemon or feature (in ML jargon) e.g. asir tempurasture or air pressure. We then extract a subset of that data covering the United Kingdom and parts of the North Atlantic and North Sea. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0318f-95ea-494c-ac41-cdfa684f3114",
   "metadata": {},
   "source": [
    "The UK subset of ERA5 data is available from zenodo. See the [readme file](README.md) file for details. You can download the data into the default location of `~/data/ukrse2022` or you can specify the location in the environment variable `${RSE22_ROOT_DATA_DIR}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da23b2-a3d7-44f2-ad38-b67d9f1e7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_global_extract:\n",
    "    print('do global_extract')\n",
    "    era5_global_cubeList = iris.load(map(str, era5_global_paths))\n",
    "    iris.util.equalise_attributes(era5_global_cubeList)\n",
    "    mslp_era5_cube = iris.cube.CubeList.concatenate_cube(era5_global_cubeList)\n",
    "    mslp_era5_uk_cube = mslp_era5_cube.intersection(latitude=uk_na_bounds['latitude'], \n",
    "                                                longitude=uk_na_bounds['longitude'])\n",
    "else:\n",
    "    print('loading cached data')\n",
    "    if load_from_zenodo:\n",
    "        mslp_era5_uk_cube = xarray.load_dataarray(zenodo_record_root  + era5_mslp_uk_subset_fname).to_iris()\n",
    "    else:\n",
    "        mslp_era5_uk_cube = iris.load_cube(str(era5_data_dir / era5_mslp_uk_subset_fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e11e8-21ca-423f-9007-8674210dea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mslp_era5_uk_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e613eb7-fb2a-4aa4-8fc4-fb1466965a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mslp_era5_uk_cube.core_data()))\n",
    "mslp_era5_uk_cube.core_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c86932-7a3c-4b49-9d87-421ba8c1f103",
   "metadata": {},
   "source": [
    "The calculation we want to do here is remove differences between seasons. What we're interested in is pressure anomolies, that is how the pressure pattern differs from what is normal for the time of year. So we want to calculate the average pressure at each point for each season. There is a lot of data so this is a large calculation, so the large compute resources we can get access to through an appropriately resourced dask cluster, especially if we were doing this on the full ERA5 data of several decades and for a larger area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd214-9978-4a91-8120-0bc6d37f7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.iris.coord_categorisation.add_season_number(mslp_era5_uk_cube,'time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f978c34-ad8d-4a46-aaab-c8e8266e3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "mslp_uk_seasonal_mean = mslp_era5_uk_cube.aggregated_by(['season_number'],iris.analysis.MEAN)\n",
    "mslp_uk_seasonal_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e96ed-9100-418b-901a-c63f87fe5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mslp_uk_seasonal_mean.core_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869d214-0910-4034-98ec-f440a994c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_season_dict = {\n",
    "    'NH Winter': 0,\n",
    "    'NH Spring': 1,\n",
    "    'NH Summer': 2,\n",
    "    'NH Autumun': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc549936-6a7f-4f1f-8d01-928c8fa7d34f",
   "metadata": {},
   "source": [
    "Next researcher may again want to explore the content of the dataset using a nice dashboard created with holoviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d1d6f-ca6a-4780-9027-e79f15ce4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_mean(plotting_cube, season):\n",
    "    print(f'season {season}')\n",
    "    fig1 = matplotlib.figure.Figure()\n",
    "    fig1.clf()\n",
    "    ax1 = fig1.add_subplot(1,1,1,projection=cartopy.crs.PlateCarree())\n",
    "    _ = iris.quickplot.contourf(plotting_cube[season],axes=ax1)\n",
    "    ax1.coastlines()    \n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105bfa1-a47c-47e8-9d71-bcdbfde2309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_select = panel.widgets.Select(options=uk_season_dict, \n",
    "                                     value=2,\n",
    "                                     name='season'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17852158-7186-4506-aae4-dcc8a2fa5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_plotter = panel.bind(\n",
    "    functools.partial(plot_seasonal_mean, mslp_uk_seasonal_mean),\n",
    "    season=season_select,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912427b-a826-4d56-8e20-781598b5253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_dash = panel.Column(season_select, season_plotter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e0a4b-3759-45bb-aa6d-d12a767a9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_dash.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fad76-b0a8-4516-9f83-3dcbe48c2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure(figsize=(9,16))\n",
    "for ix1 in range(mslp_uk_seasonal_mean.shape[0]):\n",
    "    ax1 = fig1.add_subplot(1,4,ix1+1,projection=cartopy.crs.PlateCarree())\n",
    "    iris.quickplot.contourf(mslp_uk_seasonal_mean[ix1],axes=ax1)\n",
    "    ax1.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecfd1e-3d10-4b2f-9be6-f162e2cfe2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_flat_deseasoned = dask.array.concatenate(\n",
    "    [(mslp_era5_uk_cube.extract(iris.Constraint(season_number=sn1)).core_data() - mslp_uk_seasonal_mean[sn1].core_data()).reshape(\n",
    "    (-1, mslp_era5_uk_cube.shape[1] * mslp_era5_uk_cube.shape[2])) for sn1 in range(4)],\n",
    "    axis=0\n",
    ")\n",
    "era5_flat_deseasoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c8031-6a29-4492-9543-f8788229c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_flat_deseasoned = era5_flat_deseasoned.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee55a13-592d-4423-b5ea-7e174231c623",
   "metadata": {},
   "source": [
    "## Example 3 - Creating a catalog with Intake\n",
    "\n",
    "When you think of a dataset, you probably think of a series of files in some format (often NetCDF files for weather and climate data, maybe [FITS](https://en.wikipedia.org/wiki/FITS) in astronomy or [BAM files](https://samtools.github.io/hts-specs/SAMv1.pdf) in genetics. Researchers in general don't care about the files themselves, but rather getting the data they contained loaded in memeory. There are now better options for presenting data than just a unorder collection of files. One such option is a *catalog*. The idea here is the curated data files files are presented logically in terms of the different datasets, which each logical dataset presented as a single object, irrespective of how the data is actally stored on disk. There could be one file or many files, or it could be coming from a database or an API, the researcher sees a dataset, which can then be requested and loaded into memory (either actually loading the data or setting up a stub for subsequent lazy loading and loazy execution).\n",
    "\n",
    "Here we create a very basic catalog of the data we would like to make available to researchers using the data catalogue tool called *intake*. This is a very lightweight framework for creating catalog. There are two parts to such a catalog:\n",
    "\n",
    "* Catalog description - A yml file describing what is in the dataset available through the catalog\n",
    "* Data Driver - A script which knows how to load the data in questin and return an in memory data object. This is generally provided for many existing popular data formats. It is also fairly easy to write one's own driver for new data formats (for example environmental data formats are well supported, but perhaps medical data formats are less well supported). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4969f9-8c14-42c9-9c5e-b6fec5f893c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0325c-6f84-4527-89ba-fec74e232f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_catalog = intake.open_catalog(rse_root_data_dir / 'rotors_catalog.yml')\n",
    "rotors_catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d55b3-f698-4449-9620-e1313f613a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rotors_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b3021-4b10-4f30-a0e3-56611968a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_catalog['rotors'].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37676962-5b80-4504-902f-8180710df111",
   "metadata": {},
   "source": [
    "Our catalog could have been pointing to any source of data, the details are abstracted by the catalog software, in this Intake. Now we will take quick look at programmatically creating a catalog from one or more data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fc329-be5f-4881-b31b-8596e381e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35464e52-6af1-4876-bd47-28004fec7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc1b70-135a-4601-a85c-49a8fa5723cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_fname = 'my_catalog.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754b9b3-ca5c-4749-b5a3-63527bdbcef4",
   "metadata": {},
   "source": [
    "Here we use the fsspec library to open our file. fsspec abstracts away the details of the underlying file system, so you can use the same code to read/write file on a local storage system as on a cloud object store such as [AWS S3](https://aws.amazon.com/s3), [Google Cloud storage](https://cloud.google.com/products/storage) or [Azure Blob Storage](https://azure.microsoft.com/en-gb/services/storage/blobs). The details of how to load the file are taken care of by fsspec. This combined with a catalog makes it much easier for researchers to access datasets without worrying about the technical details of storage and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49690bf4-ece7-44ab-89c1-dfb91a653982",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.open_csv(fsspec.open_local(rotors_path)).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaaac56-cd20-41ce-bcf7-4c82ab87fdaa",
   "metadata": {},
   "source": [
    "Now we can try to programmatically create a catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d12385-743f-4297-9b4b-c3afccef6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.source.csv.CSVSource(fsspec.open_local(rotors_path)).yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da242f4-cd39-461b-bff2-f5ad748f5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_cat_fname , 'w') as cat_out:\n",
    "    constructed_catalog = intake.open_csv(fsspec.open_local(rotors_path))\n",
    "    constructed_catalog.description = '''Tabular dataset with observed and simulated weather data, \n",
    "    intended for use training a machine learning model predicting turbulent wind gust events.'''\n",
    "    catalog_txt = constructed_catalog.yaml()\n",
    "    print(catalog_txt)\n",
    "    cat_out.write(catalog_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b92c4f-bd0d-462d-9071-43ef5a81b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_catalog = intake.open_catalog(new_cat_fname)\n",
    "new_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ddf43-f404-41a7-8cb1-1084c17e53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_catalog['csv'].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b79ceb-5d87-4de8-8c57-c1509507ff12",
   "metadata": {},
   "source": [
    "* https://intake.readthedocs.io/en/latest/quickstart.html\n",
    "* http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/intake.html \n",
    "* https://medium.com/informatics-lab/archive-how-to-build-an-intake-catalog-c49182297572"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557aad3-d2f0-4fef-86c5-93d7b6f67bc4",
   "metadata": {},
   "source": [
    "### Next Steps and further reading\n",
    "\n",
    "Here we have seen how we can use assorted tools an practices to make the data preparation part of a machine learning project more scalable and  reproducible, and also make the datasets created more FAIR through the use of catalogs to abstract away the details of data handling from a researcher who doesn't and shouldn't need to care about the technical details. We have used these tools in quite basic way, there are many more advanced options and tools that implement the same principles. Similarly there examples are very weather and climate focused as well as python centric. Different tools or languages will be used for different research areas (e.g. R and R Shiny may be popular)\n",
    "\n",
    "* Scaling data pipelines with dask - [docs](https://www.udemy.com/course/scalable-data-analysis-in-python-with-dask/) [Udemy course](https://www.udemy.com/course/scalable-data-analysis-in-python-with-dask/)\n",
    "  * Machine Learning with dask - [docs](https://ml.dask.org/)\n",
    "* Building a dashboard with Holoviz - [tutorial](https://holoviz.org/tutorial/exercises/Building_a_Dashboard.html)\n",
    " * Building a dashboard in R with Shiny - [docs](https://shiny.rstudio.com/articles/dashboards.html)\n",
    "* Supporting FAIR data - [European Commission report](https://ec.europa.eu/info/sites/default/files/turning_fair_into_reality_0.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df79eeb-4638-4b97-b3ea-082b52b9092a",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/docs/)\n",
    "* [dask](https://docs.dask.org/en/stable/)\n",
    "* [Iris](https://scitools-iris.readthedocs.io/en/stable/)\n",
    "* [Holoviz](https://holoviz.org/)\n",
    "* [intake](https://intake.readthedocs.io/en/latest/)\n",
    "* [fsspec](https://filesystem-spec.readthedocs.io/en/latest/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
