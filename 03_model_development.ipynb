{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1231605-17ec-49e6-8ba5-af51d54a8c15",
   "metadata": {},
   "source": [
    "# MLOps for RSEs - 3. Model Development\n",
    "\n",
    "In the first notebook, we looked at setting up a data pipeline to preprocess our data ready to use with a machine learning pipeline where we create and train machine learning models to do some prediction for our example problem. In this notebook we will look at the key principles and tools from the point of a Research Software Engineer for building such a machine learning model training pipeline. The key idea is that machine learning model are assets to be tracked and shared like code and data. Users of the models need to know all the details on how they were trained, including the input data, the architecture of the model, the hyperparameters used and many other details so that the results can be understood and trusted. This is just the same as RSEs routinely do for code, so in many ways much of what is in this notebook is not new in principle, but may be implemented slightly differently at times to handle different aspects of ML models.\n",
    "\n",
    "![Met Office Logo](https://www.metoffice.gov.uk/webfiles/1661941781161/images/icons/social-icons/default_card_315.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198c537-2a51-484a-93cd-e42fc3107adb",
   "metadata": {},
   "source": [
    "## How to train your ~~dragonüêâ~~ machine learning model ü§ñ (reproducibly, efficiently, at scale)\n",
    "Training a machine learning model is easy, at least as of 2022 when there are many libraries and tools available and a lot of good documentation and tutorials to help researchers get started in applying machine learning to their problem if they've not done it before. So in general training a small machine learning model is not something that requires special RSE support. As with many research tasks, the challenges appear when you try to take your toy example trained on a small initial dataset and try to scale it up to a large complex real world dataset. Suddenly the technical infrastructure demands of machine learning at scale become apparent and this where the expertise of a RSE becomes valuable in setting up the tools and infrastructure to easily scale up the compute to match demands of the research. \n",
    "\n",
    "In addition in a research context one is not training a single large model with well defined parameters, all the elements of the pipeline are part of the research. Researchers will likely want to perform many experiments training different models with slightly altered inputs and hyperparameters. The amount of data and other assets produced by such ML experiments can quickly become overwhelming. *Experiment tracker* software is vital, especially in a research context, to manage and compare the results of different machine learning experiments and select the best results.\n",
    "\n",
    "It is also important in larger models to be be able to track the performance of models whiles they are training to see whether progress in improving performance with respect to key metrics is being made by continuing training or whether improvements have stalled. Training dashboard tools are useful for researchers to be able to check on the status of models during what can be quite a long training process.\n",
    "\n",
    "The steps for this part of the project are typically as follows:\n",
    "* *create train/validate/test splits* - a key practice in train ML models is to only train with a portion of the data, and hold back smaller portions of the data for for evaluating model performance both during the development cycle (validate or dev split), and right at the end of the project to check the final solution generalises to unseen data (test split).\n",
    "* *prepare the data* - Although most of the work  will likely have been done during the data prep part of the project, there is likely at least a small amount of \"last mile\" preparation to prepare the data for use with the particular ML framework and architecture being used. In particular normalising the data.\n",
    "* *build the model architecture* - Use the chosen modelling framework (e.g. TensorFlow, PyTorch) to set up the model.\n",
    "  * *log hyper parameters* - log the details of the model architecture and any hyperparameters with experiment tracking tool to be able recreate the architecture later.\n",
    "* *train the model* - Run the training algorithm to determine the parameters for the model that best fit the training data.\n",
    "  * *monitor training* - Training is usually the longest and most compute intensive part of the process for most machine learning projects. While it is ongoing, there are various tools to monitor the current state of the model, so you can abort training if training is going wrong and generally check if it is converging and loss is decreasing.\n",
    "  * *log training metrics* - Either during training or afterwards, log the performance metrics for the model with the experiment tracking tool.\n",
    "* *save the the trained model*  - Once the model is trained, you want to save the definition of the architecture and the weights to some sort of persistent storage for subsequent inference.\n",
    "  * *log trained model* with experiment manager* The architecture and weights/thresholds can also be saved with as artifact of the run in the experiment tracking tool so that all the configuration and outputs of a particular run are accessible in the same place.\n",
    "\n",
    "A key part of model development is finding the correct *hyperparameters* for the model. Hyperparameters are the configuration elements of the model that are not determined by training the model. Those elements that are calculated, for example the weights and coefficients of a neural network, are called parameters. Those that are not, for example the number of hidden layers are learning rate for a neural network, are called hyperparameters. Hyperparameters are usually found by trial and error for specific problems. An additional training loop to find the best hyperparameters is usually employed and this is called hyperparameter tuning. This is an inherently parallel process which involves training many models independentally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cf29b-459d-41f9-a666-12e05ae8f183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Key Principles\n",
    "* *reusability* - ensure the elements of training infrastructure are easy to reuse/adapt for different models and projects.\n",
    "* *scalability* - ensure researchers can select the size of training dataset and the size/complexity of the model architecture to match their requirements, rather than having to restrict the scale of their experiments due to technical limitations.\n",
    "* *reproducibility* - providing tools to systematically record the inputs, configurations and outputs of model training run to keep track of experiments so results can be reproduced and evidence compiled for reports and papers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9874d17-7d00-44c5-ac13-96ebe1800029",
   "metadata": {},
   "source": [
    "### Key Tasks for RSEs\n",
    "* Setting up infrastructure for training models\n",
    "* Facilitating running code on ML targeted platforms e.g. GPU, TPU etc.\n",
    "* Support good practices for ML development e.g. experiment tracking\n",
    "* Applying FAIR principles to all ML assets (data, code, trained models)\n",
    "* Applying good code management practices to ML code\n",
    "* Setting up test suites for ML pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735e575-ce92-4c38-b6d0-555f270a1c8a",
   "metadata": {},
   "source": [
    "### Key Terms\n",
    "* *experiment tracking* - a place to log all of the configuration and outputs from a machine learning experiment in one place, allowing the researchers to easily compare results from different runs, select the best run and compile evidence for papers,reports and other research outputs.\n",
    "* *machine learning pipeline/workflow* - A series of tasks centred around training and evaluating machine learning models.\n",
    "* *train/test split* - Splitting the available data for training a model into separate sets so that some of the data is used to train the model and the rest, which the model will not have been exposed to in training, will be used for an evaluation on generalization capability of the model to independent data.\n",
    "* *training* - The process of finding the best *parameters* for a model with the specified architecture that best fits the training data.\n",
    "* *hyper parameters* - Hyper parameters are part of the configuration of the model that is not determined by training. These include the structure or architecture of the model, such as the number of hidden layers or number of nodes in a layer for a neural network, or the depth of decision tree.\n",
    "  * *hyper parameter tuning* - As hyperparameters are not optimised as part of the training process, a separate outer training loop, called hyperparameter tuning, is run to automate the search for the optimal hyperparameters for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986fd2d-bfdb-44c6-a15b-54e7b05c338b",
   "metadata": {},
   "source": [
    "### Key Tools\n",
    "* ML frameworks (scikit-learn, TensorFlow, PyTorch)\n",
    "* Workflow tools (ray)\n",
    "* Experiment tracking (MLflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec43b8-57c5-46fc-9ab4-d367015734a9",
   "metadata": {},
   "source": [
    "### Running this notebook\n",
    "This notebook should run from a conda environment created with the [requirements_model_development.yml file](requirements_model_development.yml). See the [readme file](https://github.com/informatics-lab/ukrse_2022_mlops_walkthrough/blob/main/README.md) for info on how to set up a conda environment for using this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f308962-12aa-40ea-a996-ba2df9341e32",
   "metadata": {},
   "source": [
    "## Example problem - Predicting wind rotors\n",
    "\n",
    "In the previous notebook, we introduced the example problem of predicting wind rotor events using machine learning. Now we are going to train a model to do this prediction for that problem. We will use this example problem to demonstrate the following parts of training a model:\n",
    "* loading the preprocessed data from an Intake catalog.\n",
    "* set up an experiment and run in our experiment tracking tool to track and store the configuration and outputs of our experiment using ML Flow.\n",
    "* monitor the training process using TensorBoard (not particularly necessary for such a small example of course, but a lot more valuable for larger models and datasets.).\n",
    "* save the model for subsequent inference in notebook 4 (model evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a01caf-2fd1-4509-8ee0-5819b391767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8319b23-ecaf-4acf-9919-9e1b4b745ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import datetime\n",
    "import os\n",
    "import functools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45036d-70ac-4569-96a0-1772efca8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccbe28e-3dfd-4092-a138-17fc1696a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cc24c-ded1-40b2-a0cb-7641ea5b4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe869b-e80e-407b-b8d4-a06983bbf8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.layers\n",
    "import tensorflow.keras.models\n",
    "import tensorflow.keras.optimizers\n",
    "import tensorflow.keras.metrics\n",
    "import tensorflow.keras.layers\n",
    "import tensorflow.keras.constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524f2f1-23f9-49c4-8ce1-2efc9c428dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5f0b6-3366-4fbd-8fda-49eaa08ff142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc663b-930b-4d87-b499-a5ec9a7abb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49eccb8-404f-4f7b-ae35-9eca57818712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b63f8f-4259-49ad-8a61-eb141b663dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_dash_port = 5001\n",
    "tensorboard_dash_port = 5002\n",
    "ray_dash_port = 5003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132d5b-aaac-4336-b1ab-ac8a6108ba95",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "In the previous notebook we explored the data and prepared it for use in training a machine learning model. We saved the resulting dataset in a catalog, and now we will use that catalogue to load the data. The advantage of this approach is that it is easy for other to find this dataset by looking at the catalogue. For example you could imagine a common catalogue for a research group to allow members to find datasets they can use for their projects. This is useful not just for ML projects. In addition, catalogs support  separation of tasks and responsibilities within a project and team, so those responsible for creating/curating the data make it available through the catalogue, then others who are responsible for the ML modelling aspects of project can just access the data from the catalogue and get on with their part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a680df-9452-4a0f-96a9-64c326ffafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rse_root_data_dir = pathlib.Path(os.environ['RSE22_ROOT_DATA_DIR'])\n",
    "    print('reading from environment variable')\n",
    "except KeyError as ke1:\n",
    "    rse_root_data_dir = pathlib.Path(os.environ['HOME'])  / 'data' / 'ukrse2022'\n",
    "    print('using default path')\n",
    "rse_root_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f646b1-d07d-42cd-9045-aac9833a7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_catalog = intake.open_catalog(rse_root_data_dir / 'rotors_catalog.yml')\n",
    "rotors_catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb98706-50ab-4c63-85fe-80571bbe66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rotors_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4f23f-455e-4638-9fd3-75e83d7c6fab",
   "metadata": {},
   "source": [
    "We see that our catalog contains preprocessed data ready to use in our machine learning development pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23aba47-9061-487a-bc64-1358abc0ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df = rotors_catalog['rotors_preprocessed'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68273f-ce70-4346-8af0-c119905029ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da202c-518e-49eb-ab2e-7b0066dfa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rotors_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164a8b4-fb1c-4c08-b3bd-919e0f545228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one small bit of cleaning: ensuring the correct datetime type for our time feature\n",
    "rotors_df['time'] = pandas.to_datetime(rotors_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cd664-d0c1-4e22-9e09-2808ebeddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]\n",
    "u_wind_feature_names = [f'u_wind_{i1}' for i1 in range(1,23)]\n",
    "v_wind_feature_names = [f'v_wind_{i1}' for i1 in range(1,23)]\n",
    "target_feature_name = 'rotors_present'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de9bdc-8f6b-4264-923d-bfbe645503b5",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "\n",
    "The next important part in preparing the data is to split it into parts with some used for training and some used for evaluation. The best practice for this is to split the data into three parts:\n",
    "* training - This is the data that will be shown to the algorithm during training\n",
    "* validation/dev - This is data set aside that is used to evaluate the trained model during active development of the model. Based on this performance you may choose to update the hyperparameters or refine/augment the dataset to improve results. \n",
    "* test - this part consists of data that put aside until the very end of the project. This ensures that neither the model parameters not the hyperparameters are overfitted or too specifically tuned to the particular contents of the data and will actually generalise to the broader problem being solved.\n",
    "\n",
    "For this example, we're just using train and test to keep things simple. Often train/validate/test sets are chosen at random. Sometimes though, specifically for time series or geospatial data, neighboring points can be correlated in some way, so typically more care is required. In this case we are going to split our data by year to avoid correlations between train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea031e48-ad63-49bd-ae5e-247f2faa0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rotors_df[rotors_df['time'] < datetime.datetime(2020,1,1,0,0)]\n",
    "val_df = rotors_df[rotors_df['time'] > datetime.datetime(2020,1,1,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a795db-7530-4172-90eb-40f752d97b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_names = temp_feature_names + humidity_feature_names + u_wind_feature_names + v_wind_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5daf4-bacf-4485-aa1a-0269414e20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict = {}\n",
    "for if1 in input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(train_df[[if1]])\n",
    "    preproc_dict[if1] = scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493be9a-8cba-45de-a292-19ca89c9642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(train_df[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76262906-8df9-426c-98aa-2838eaa4aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_input(data_subset, pp_dict):\n",
    "    return numpy.concatenate([scaler1.transform(data_subset[[if1]]) for if1,scaler1 in pp_dict.items()],axis=1)\n",
    "\n",
    "def preproc_target(data_subset, enc1):\n",
    "     return enc1.transform(data_subset[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30332d1-3955-49d6-94a2-82af6642ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preproc_input(train_df, preproc_dict)\n",
    "y_train = numpy.concatenate(\n",
    "    [preproc_target(train_df, target_encoder).reshape((-1,1)),\n",
    "    1.0 - (preproc_target(train_df, target_encoder).reshape((-1,1))),],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2064537-abc2-4d19-b2a1-dc0516fbc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = preproc_input(val_df, preproc_dict)\n",
    "y_val = numpy.concatenate(\n",
    "    [preproc_target(val_df, target_encoder).reshape((-1,1)),\n",
    "    1.0 - (preproc_target(val_df, target_encoder).reshape((-1,1))),],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a262a54-9489-480e-947f-544627129cbb",
   "metadata": {},
   "source": [
    "### Set up experiment tracking - ML Flow\n",
    "Now that we've loaded and prepared our data, we can start training some machine learning models! Over the course of a project, we may want to train many models, varying different elements of the training configuration, model architecture, training data and many other aspects. Experiment tracking helps us log a complete description of the experiment so we can compare different results, select best configurations and be able to reproduce results when required.\n",
    "\n",
    "An important distinction of concepts in experiment tracking frameworks generally, and specifically for ML Flow, is between experiments and runs. An experiment is a particular setup of your machine learning pipeline that you are wanting to develop and optimise. Within each experiment you are likely to have many runs, representing each iteration of a particular configuration. \n",
    "https://www.mlflow.org/docs/latest/concepts.html\n",
    "\n",
    "In this example, we will use an ML Flow server running locally.  To start up such an ML Flow server, activate the conda environment used for this notebook, then run the following ML Flow command from the command line to start your tracking server\n",
    "\n",
    "```mlflow server --port $PORT --backend-store-uri sqlite://<PATH TO DB>/<FILENAME>.db  --default-artifact-root $ARTIFACT_PATH```\n",
    "\n",
    "To Note\n",
    "\n",
    "* This will only be accessible on your local machine. Yo make more widely accessible use the `--host 0.0.0.0` option\n",
    "* when specifying the backend URI, if you specify a full path you will need FOUR slashes after the sqlite:, for example sqlite:////user/name/experiments/my_project.db, otherwise you will get an error.\n",
    "* The artifact store can be a local file path e.g. /path/to/artifacts/ or a remote object store like S3 e.g. s3://project_bucket/project_key/\n",
    "\n",
    "We will now set up our access to our server and set up an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ea948-abfb-4954-8d39-dc24c818ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_rotors_experiment_name = 'rse_mlops_demo_rotors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d8e79-f46a-4008-a3a1-97589a93524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_template = '{dt.year:04d}{dt.month:02d}{dt.day:02d}T{dt.hour:02d}{dt.minute:02d}{dt.second:02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054dfce5-65ea-41f2-9772-eab4dcda050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_run_name_template = 'rse_rotors_{network_name}_' + timestamp_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5e3f4-f6c6-4d9b-b7b8-dccfb861ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_server_address = '127.0.0.1'\n",
    "mlflow_server_port = mlflow_dash_port\n",
    "mlflow_server_uri = f'http://{mlflow_server_address}:{mlflow_server_port:d}'\n",
    "mlflow_server_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06a066-4875-42b6-9efa-9f7d6e32336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(mlflow_server_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d516e-933a-490f-9f90-ef1905d4e0cb",
   "metadata": {},
   "source": [
    "Example of required run: ```mlflow server --port 5001 --backend-store-uri sqlite:///mlflowSQLserver.db  --default-artifact-root ./mlflow_artifacts/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449510d2-c0b6-413d-83a3-4c0aa8faccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    print('creating experiment')\n",
    "    rse_rotors_exp_id = mlflow.create_experiment(rse_rotors_experiment_name)\n",
    "    rse_rotors_exp = mlflow.get_experiment(rse_rotors_exp_id)\n",
    "except mlflow.exceptions.RestException:\n",
    "    rse_rotors_exp = mlflow.get_experiment_by_name(rse_rotors_experiment_name)\n",
    "rse_rotors_exp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3a9f7-bd95-4d69-b380-a2c4122c2f97",
   "metadata": {},
   "source": [
    "### Set up model architecture hyperparameters\n",
    "\n",
    "Now we will actually set up our models for this pipeline. In this case we're using a fairly simple feed-forward neural network. We don't need to do any specific logging of config or architecture at this point because the autologging capabilities of ML Flow for TensorFlow/Keras will ensure all these hyperparameters are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8169a5-f82b-48ab-9011-8e8a535f3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ffnn_model(hyperparameters, input_shape):\n",
    "    \"\"\"\n",
    "    Build a feed forward neural network model in tensorflow for predicting the occurence of turbulent orographically driven wind gusts called Rotors.\n",
    "    \"\"\"\n",
    "    model = tensorflow.keras.models.Sequential()\n",
    "    model.add(tensorflow.keras.layers.Dropout(hyperparameters['drop_out_rate'], \n",
    "                                              input_shape=input_shape))\n",
    "    for i in numpy.arange(0,hyperparameters['n_layers']):\n",
    "        model.add(tensorflow.keras.layers.Dense(hyperparameters['n_nodes'], \n",
    "                                                activation=hyperparameters['activation'], \n",
    "                                                kernel_constraint=tensorflow.keras.constraints.max_norm(3)))\n",
    "        model.add(tensorflow.keras.layers.Dropout(hyperparameters['drop_out_rate']))\n",
    "    model.add(tensorflow.keras.layers.Dense(2, activation='softmax'))             # This is the output layer\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40187a-e968-4b01-bd16-5a387d58a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = X_train.shape[1]\n",
    "input_shape = (nx,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448fc6d-7605-426f-8750-2ada22900ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_dict = {\n",
    "    'initial_learning_rate': 1.0e-4,\n",
    "    'drop_out_rate': 0.2,\n",
    "    'n_epochs': 100,\n",
    "    'batch_size': 1000,\n",
    "    'n_nodes': 1000,\n",
    "    'n_layers': 4,\n",
    "    'activation': 'relu',\n",
    "    'loss': 'mse'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8ad01-ed3a-45d0-a29a-582582a32f11",
   "metadata": {},
   "source": [
    "### Train model and setup monitoring\n",
    "\n",
    "Now we're (almost) all setup to run our training. To monitor while training is running we'll use tensorboard, so we'll set up tensorboard ready to use. TensorBoard uses a callback in the Keras training loop to update the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a74806-91be-4d71-97b9-42684f279fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_tensorboard = rse_root_data_dir / 'log_tensorboard' \n",
    "if not log_dir_tensorboard.is_dir():\n",
    "    log_dir_tensorboard.mkdir()\n",
    "    print(f'created tensorboard log directory {log_dir_tensorboard}')    \n",
    "log_dir_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61371d9c-1e0e-4f69-8ccc-ab164edb5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard, \n",
    "                                                              histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250970b6-eb9c-4153-a55d-0e43162465b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba04066-0ee9-41c2-a959-0371ca99267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ~/data/ukrse2022/log_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446885b6-ced8-4d6b-98c5-b92f43d417f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "current_run_name = rse_run_name_template.format(network_name='ffnn',\n",
    "                                                dt=datetime.datetime.now()\n",
    "                                               )\n",
    "with mlflow.start_run(experiment_id=rse_rotors_exp.experiment_id, run_name=current_run_name) as current_run:\n",
    "    rotors_ffnn_model = build_ffnn_model(hyperparameters=hyperparameters_dict,\n",
    "                                     input_shape=input_shape,\n",
    "                                    )\n",
    "    rotors_ffnn_optimizer = tensorflow.optimizers.Adam(\n",
    "        learning_rate=hyperparameters_dict['initial_learning_rate'])  \n",
    "    \n",
    "    rotors_ffnn_model.compile(optimizer=rotors_ffnn_optimizer, \n",
    "                          loss=hyperparameters_dict['loss'], \n",
    "                          metrics=[tensorflow.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    history=rotors_ffnn_model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        validation_data=(X_val, \n",
    "                          y_val), \n",
    "        epochs=hyperparameters_dict['n_epochs'], \n",
    "        batch_size=hyperparameters_dict['batch_size'], \n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[tensorboard_callback],\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccfdf7-b7fa-4310-bb19-e2cad0f9498a",
   "metadata": {},
   "source": [
    "Once our training has run, you can then take a look at the outputs in ML Flow to see all the aspects of logged. In this notebook we can look at our ML Flow server at http://127.0.0.1:5001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecc5e3-253f-4649-9a05-83edcdf81c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_rotors_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7bb2b-94f6-4058-8094-052734c8f96f",
   "metadata": {},
   "source": [
    "##  Example - Hyperparameter tuning in ray\n",
    "\n",
    "As explained in the introduction to this notebook, the processing of training a machine learning algorithm calculates the *parameters* of the model. There are additional config values not calculated by the training process that are determined by the data scientist doing the training which are called hyperparameters. The values of these hyperparameters are typically problem dependant and so change from problem to problem, even within a project. While over time one may develop an intuition for what hyperparameter values are suitable in a particular context, typically these values are determined in an outer training loop, called hyperparameter tuning.\n",
    "\n",
    "In hyperparameter tuning, many models are trained, each with different hyperparameters. Common ways to select candidate hyperparameter combinations are through a grid search through the n-dimensional hyperparameter space, or specifying possible hyperparameter values as independent distributions that are randomly sampled n times for n trials. Typically the hyperparameters for the trained model that scores best on the particular metric of choice will be chosen for the particular problem from then on.\n",
    "\n",
    "As each of these trials is completely independent, it is an easy workflow (conceptually at least) to execute in parallel. One can actually do this entirely manually by just setting up many different training runs with different hyperparameter runs. There are many ways to make mistakes in gathering and selecting the best results and saving output for evidence, as well as easily scaling up to run these trials in parallel etc. It is best to use existing tools to facilitate scaling and reproducibility.\n",
    "\n",
    "In this example, we are now going to do hyperparameter tuning using the *Ray* library, which is a python library for facilitating parallel, distributed computing. In this example we are just going to run on a local cluster to make thing easier for this tutorial, but ray provides facilities to easily set up a cliuster of ray worker on a linux cluster or a cloud VM  or a kubernetes cluster or almost any distributed computing platform.\n",
    "\n",
    "Facilitating such use of tools and compute resources is where RSEs add value to a project so that research requirements determine technical implementation, not the other way around, and ensuring the best trained ML model is used to produce the best research outputs.\n",
    "\n",
    "Ray docs:\n",
    "* [ray tune docs](https://docs.ray.io/en/latest/tune/index.html)\n",
    "* [source for example](https://docs.ray.io/en/latest/tune/examples/tune_mnist_keras.html#tune-mnist-keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1a204-96ba-4ab7-8c22-555e14308dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rotors_hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f07c8-1659-4829-8fd9-b14d0f911c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.tune\n",
    "import ray.tune.schedulers \n",
    "import ray.tune.integration.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deddd6-9c2f-4a84-9c5c-6e9fe1b9938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_terations=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2dd815-837c-432f-83c4-749393fa49fe",
   "metadata": {},
   "source": [
    "Here we initialise ray (this should only be called once), so that we can view the ray dashboard to monitor progress on the specified URL `http://127.0.0.1:5003`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a404e5-1ffc-4618-a4ec-6e97f528aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=4, dashboard_port=ray_dash_port, dashboard_host='127.0.0.1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ae45a-9d36-49af-a8dc-1ca00a504738",
   "metadata": {},
   "source": [
    "Now we specify distributions for the hyperparameters we want to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1288fd5-6825-49e2-937d-f3804f7c702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_hpt_config = {\n",
    "    'initial_learning_rate': ray.tune.uniform(1e-5,1e-3),\n",
    "    'n_nodes': ray.tune.randint(100,500),\n",
    "    'n_layers': ray.tune.randint(2,6)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b99fb8-1005-43db-b36d-462cbb3743b9",
   "metadata": {},
   "source": [
    "We then set up a hyperparameter job that will execute on our local ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161cc4d-91ba-4c6f-9c19-32480010a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_hpt_analysis = ray.tune.run(\n",
    "    rotors_hpt.run_ml_pipeline,\n",
    "    name=\"rse_rotors_hpt\",\n",
    "    scheduler=ray.tune.schedulers.AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\", \n",
    "        max_t=400, \n",
    "        grace_period=20,\n",
    "    ),\n",
    "    metric=\"root_mean_squared_error\",\n",
    "    mode=\"max\",\n",
    "    stop={\"root_mean_squared_error\": 0.99, \n",
    "          \"training_iteration\": num_training_terations},\n",
    "    num_samples=10,\n",
    "    resources_per_trial={\"cpu\": 2,\n",
    "                         \"gpu\": 0},\n",
    "    config=rotors_hpt_config,\n",
    "    progress_reporter=ray.tune.JupyterNotebookReporter(overwrite=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609aa66-09c7-4cdc-8644-8513d07cb814",
   "metadata": {},
   "source": [
    "Once the job has run, we can then interrogate the results and find the best hyperparameters found from our search. In this case we've done quite a small search so we might not have found the best values, as this is a more illustrative example of how to tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5a15c-ae7d-4b48-8a3d-c01232411fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([f'{k1} - {v1[\"root_mean_squared_error\"]}' for k1,v1 in rotors_hpt_analysis.results.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da2f2d-7a82-49c3-8a7e-f1fb97ed9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_hpt_analysis.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd2afc-50a3-4a54-b675-cb0672f3580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_hpt_analysis.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039815cb-a76e-4a53-a668-bf8eeddbc292",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_hpt_analysis.best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b6755-5eef-473d-85f7-8f39bcc0fda6",
   "metadata": {},
   "source": [
    "# Example - save the model \n",
    "\n",
    "Having trained a model, possibly through hyperparameter tuning, we want our model to persist and so we need to store the trained weights and the architecture do describe the structure of the model to disk in a file of some sort. This is the first step in treating our models as *machine learning assets* that like data and code should follow *FAIR* principles.\n",
    "\n",
    "We will explore several way to save our models. The first is to use the ML framework we're using for our model, in this case the Keras interface to TensorFlow. Secondly we'll use ML Flow, the experiment tracking tool which provides a framework independent way to save and load models. thirdly we'll look at a completely separate tool which is intended as an open standard way for exchanging machine learning models called *Open Neural-network eXchange* format or ONNX.\n",
    "\n",
    "Docs:\n",
    "* [Keras Model Saving/Loading](https://www.tensorflow.org/guide/keras/save_and_serialize)\n",
    "* [ML Flow Models](https://www.mlflow.org/docs/latest/models.html)\n",
    "* [ONNX](https://onnxruntime.ai/docs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14811d5-0326-4396-87b6-9032f1d55119",
   "metadata": {},
   "source": [
    "### Keras model format\n",
    "\n",
    "First we'll use the framework specific Keras format to save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc020b2-c18a-403e-a00c-1ce7a34a6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_ffnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11364535-21fa-4bbe-a54d-6087099289e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras_export_path = rse_root_data_dir / 'keras_model'\n",
    "model_keras_export_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe69b1-c7bf-4114-a634-c96eade78743",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_ffnn_model.save(model_keras_export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309d7b9-2c7b-4764-9f64-2b7d25af3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_keras_export_path.rglob('*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24285fe-aace-4ea1-bc27-25da8e6ea4ec",
   "metadata": {},
   "source": [
    "### ML FLOW\n",
    "\n",
    "When we trained our model at the start of the model, we logged all the details using ml flow automatically because at the start of the notebook we called `mlflow.tensorflow.autolog()` so that MLflow automatically logged any calls to `fit` with a TensorFlow model. So as we'll see in the next notebook, we can access our saved models through the runs that are logged with ML Flow without explicitly saving the model. We can also do an explicit save outside the default artifact store of our ML Flow server as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3210a1f-5a4b-44a1-bb87-229fa7705bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_model_path = rse_root_data_dir / 'mflow_model'\n",
    "mlflow_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdda7ea-62fd-4a77-ad01-5dee6b225625",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(mlflow_model_path.rmdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520bdfb-f1d4-4713-9545-11670092ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b65b6-0402-47f0-8522-b431e16f61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow_model_path.is_dir():\n",
    "    shutil.rmtree(mlflow_model_path)\n",
    "    print('deleted existing directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b125dcc-fd3e-470d-8616-6475ca64adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.keras.save_model(rotors_ffnn_model, mlflow_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0daaec3-b2bd-4029-a9e4-811e0abe2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mlflow_model_path.rglob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75db94-5ef7-490e-8784-5df9de4add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.keras.load_model(str(mlflow_model_path) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a48a3d-1ad6-4b86-82d4-5274d41b4d5a",
   "metadata": {},
   "source": [
    "### ONNX\n",
    "\n",
    "The third format is a standard alone library for storing models in a common format and a runtime environment for inference that is independent of the framework that trained the models. This should (potentially) provide a more stable environment for model storage and inference that does not need to change if the model training pipeline and its dependencies change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fc5a7-b491-4bae-8ac8-c6b1b0ea9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4747c2-edb9-42aa-b0c3-4dab3f1fd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf2onnx.convert.from_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1b878-e328-413e-b844-098a7317dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rotors_onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    rotors_ffnn_model,\n",
    "    [tensorflow.TensorSpec(\n",
    "        shape=tensorflow.TensorShape([None,X_train.shape[1] ]),\n",
    "        dtype=X_train.dtype,\n",
    "        name='ukmo_rotors_model_input',\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc651e12-4662-48f3-9aab-502fbac09ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = rse_root_data_dir / 'onnx_model'\n",
    "onnx_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fb600-d992-484d-b2ce-53d9818ae04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save_model(rotors_onnx_model, str(onnx_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a886b1-801b-42ca-b7be-1ec7ee0b1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded42eb-b072-4af3-8712-3ed478cbd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning do not try to display, it will freeze your notebook!\n",
    "reloaded_onnx_model = onnx.load_model(str(onnx_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e276609-6641-4d20-a5c8-3be339f6a26a",
   "metadata": {},
   "source": [
    "### Next Steps / Further Reading\n",
    "Many of the features can be integrated into new or existing ML projects immediately and provide benefits for researchers almost immediately. You can find more more about the core concepts with links below:\n",
    "\n",
    "* [Experiment Tracking concepts](https://www.mlflow.org/docs/latest/concepts.html)\n",
    "* [Explanation of hyperparameter tuning - Google Cloud](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview)\n",
    "* [Running hyperparameter tuning at scale - Ray Tune](https://docs.ray.io/en/latest/tune/getting-started.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc446a-15c6-43e3-a556-2d191f18e432",
   "metadata": {},
   "source": [
    "### References\n",
    "* [ML Flow Models](https://www.mlflow.org/docs/latest/models.html)\n",
    "* [TensorFlow Keras](https://keras.io/about/)\n",
    "  * [Saving Keras Models](https://www.tensorflow.org/guide/keras/save_and_serialize)\n",
    "* [TensorBoard](https://www.tensorflow.org/tensorboard)\n",
    "* [ONNX](https://onnx.ai/)\n",
    "* [Ray tune](https://docs.ray.io/en/latest/tune/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukrse2022mlops_model_dev",
   "language": "python",
   "name": "ukrse2022mlops_model_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
