{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a8d9c7-2a11-42ec-ba7f-0bdc208484d8",
   "metadata": {},
   "source": [
    "# MLOps for RSEs - 4. Model Evaluation & Reuse\n",
    "\n",
    "In this third part of the tutorial, we'll look at some of the practices and tools that are used after the machine learning model is trained. These activities are generally about evaluating the performance of the model,  understanding the results it produces and sharing the outputs so the model and its predictions can be reused. The FAIR principles are hopefully well known to most RSEs, and their application to both code and data. In a machine learning-based project it is important to apply FAIR principle to machine learning model and other ML assets to promote reproducible research and general reuse of research assets.\n",
    "\n",
    "![Met Office Logo](https://www.metoffice.gov.uk/webfiles/1661941781161/images/icons/social-icons/default_card_315.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195024ac-c02d-49f4-b238-5a9c9184c890",
   "metadata": {},
   "source": [
    "### Key Principles\n",
    "* *Reusability* - making trained models, model results, performance metrics and explainability metricseasily  available to other researchers\n",
    "* *Scalability* - Be able to produce and serve results at scale, either for large datasets and problems or for many users simulataneously.\n",
    "* *Interactvity* - Make research outputs available through an interactive interface for to faclitate exploration and use of results in an intuitive way that does not distract from thinking about the actual problem the researcher is interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92781d-deb7-4747-a5b2-783b5c0c4bd9",
   "metadata": {},
   "source": [
    "### Key Tasks for RSEs\n",
    "* Setting up infrastrcuture to serve models and model results\n",
    "* Creating model evaluation and testing suites/workflows to evaluate model performance in a reproducible and comparable way\n",
    "* Supporting tools for interpretability and explainability (XAI)\n",
    "* Creating dashboard for ML outputs including\n",
    "  * predictions from inference\n",
    "  * performance metrics\n",
    "  * explainability metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414ee44-39c9-4025-8ab6-fdc67b53e365",
   "metadata": {},
   "source": [
    "### Key Terms\n",
    "* *inference* - producing predictions from the trained machine learning model\n",
    "* *Explainable AI (XAI)* - A machine learning pipeline where the predictions made by the algorithm can be explained and interpreted (what does this prediction mean in terms of the problem domain?).\n",
    "* *Explainability* - Part of XAI which enables some one to explain how a machine learning algorithm made a certain prediction, such as which inputs were important, which particular parts of a neural network or decision tree were activated etc.\n",
    "* *Interpretability* - Part of XAI which tell us what a particular prediction mean in the original problem domian and generally understanding the performance of the model in the research context.\n",
    "* *metrics* - values which characterise some aspect of the model and its results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936aae0-817f-4ec7-9811-9ca4451bd8d9",
   "metadata": {},
   "source": [
    "### Key Tools\n",
    "* model storage/loading ([mlflow](https://mlflow.org/), [onnx](https://github.com/onnx))\n",
    "  * model registry ([ML Flow models](https://www.mlflow.org/docs/latest/models.html))\n",
    "* inference ([mlflow](https://mlflow.org/), [ray serve](https://docs.ray.io/en/latest/serve/index.html))\n",
    "* workflow mangers/engines ([ray](https://docs.ray.io/))\n",
    "* metrics ([scikit learn](https://scikit-learn.org/))\n",
    "* XAI ([scikit learn](https://scikit-learn.org/), [shap](https://shap.readthedocs.io/en/latest/index.html), [omni xai](https://github.com/salesforce/OmniXAI))\n",
    "* dashboards ([matplotlib](https://www.google.com/search?q=matplotlib&oq=matplotlib&aqs=chrome..69i57j0i131i433i512j69i59l2j0i131i433i512j69i65l3.2614j0j7&sourceid=chrome&ie=UTF-8), [holoviz](https://holoviz.org/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606c3eb-ec9e-4f5d-a525-b1977e676769",
   "metadata": {},
   "source": [
    "### Running this notebook\n",
    "This notebook should run from a conda environment created with the [requirements_evaluation.yml file](requirements_evaluation.yml). See the [readme file](https://github.com/informatics-lab/ukrse_2022_mlops_walkthrough/blob/main/README.md) for info on how to set up a conda environment for using this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4debb-ccfb-4c33-8a74-f9930192c522",
   "metadata": {},
   "source": [
    "## Loading trained models for inference\n",
    "\n",
    "Once you've trained a model, or more usually in a research several different models to explore different options for you model such as different model architectures, different input features or different training datasets, you then want to evaluate the performance of each model (using performance metrics) and also understand how it is making (using interpretability and explainability techniques).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fc1e3-2937-491a-9c53-230dcfbe753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pathlib\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e691dd7-9422-41b2-935e-dc888e3aff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e7336-876a-4d44-8d62-d7d800499e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa635de-60e3-44bc-bfd3-63cb8550820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df5af6-c6d2-4ecb-b794-b6fb99c74882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4a772-f414-4a3e-9aec-457ef4a1a5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get train and test data\n",
    "\n",
    "First we load the data, both input data to feed in to the model and target data with which to compare results to assess performance. We will agin load the data from the catalog we created in the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f2246-f7ab-46aa-aecd-9c4b44af688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rse_root_data_dir = pathlib.Path(os.environ['RSE22_ROOT_DATA_DIR'])\n",
    "    print('reading from environment variable')\n",
    "except KeyError as ke1:\n",
    "    rse_root_data_dir = pathlib.Path(os.environ['HOME'])  / 'data' / 'ukrse2022'\n",
    "    print('using default path')\n",
    "rse_root_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc54fe-d501-4579-8a06-0fdf3cc10144",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_catalog = intake.open_catalog(rse_root_data_dir / 'rotors_catalog.yml')\n",
    "rotors_catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e3df9-82ad-49bc-8835-e89589f42131",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_df = rotors_catalog['rotors_preprocessed'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7839fc-4c94-4df1-a54f-7e6790ed498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one small bit of cleaning: ensuring the correct datetime type for our time feature\n",
    "rotors_df['time'] = pandas.to_datetime(rotors_df['time'])\n",
    "temp_feature_names = [f'air_temp_{i1}' for i1 in range(1,23)]\n",
    "humidity_feature_names = [f'sh_{i1}' for i1 in range(1,23)]\n",
    "wind_direction_feature_names = [f'winddir_{i1}' for i1 in range(1,23)]\n",
    "wind_speed_feature_names = [f'windspd_{i1}' for i1 in range(1,23)]\n",
    "u_wind_feature_names = [f'u_wind_{i1}' for i1 in range(1,23)]\n",
    "v_wind_feature_names = [f'v_wind_{i1}' for i1 in range(1,23)]\n",
    "target_feature_name = 'rotors_present'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81147a95-0fe3-4445-9f57-ed087a484ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rotors_df[rotors_df['time'] < datetime.datetime(2020,1,1,0,0)]\n",
    "val_df = rotors_df[rotors_df['time'] > datetime.datetime(2020,1,1,0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5dbdd-a489-467b-980f-6303cb8f7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_names = temp_feature_names + humidity_feature_names + u_wind_feature_names + v_wind_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb08596-5929-4740-be7d-68b04de0560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dict = {}\n",
    "for if1 in input_feature_names:\n",
    "    scaler1 = sklearn.preprocessing.StandardScaler()\n",
    "    scaler1.fit(train_df[[if1]])\n",
    "    preproc_dict[if1] = scaler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5351c-7dc3-499b-9ab0-41d31ea901e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "target_encoder.fit(train_df[[target_feature_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955993b-0abe-452b-8614-164e84bde621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_input(data_subset, pp_dict):\n",
    "    return numpy.concatenate([scaler1.transform(data_subset[[if1]]) for if1,scaler1 in pp_dict.items()],axis=1)\n",
    "\n",
    "def preproc_target(data_subset, enc1):\n",
    "     return enc1.transform(data_subset[[target_feature_name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a92a60-19a1-4308-8129-e425e5fd2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preproc_input(train_df, preproc_dict)\n",
    "y_train = numpy.concatenate(\n",
    "    [preproc_target(train_df, target_encoder).reshape((-1,1)),\n",
    "    1.0 - (preproc_target(train_df, target_encoder).reshape((-1,1))),],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd5a37-677b-4111-811d-568ef327f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = preproc_input(val_df, preproc_dict)\n",
    "y_val = numpy.concatenate(\n",
    "    [preproc_target(val_df, target_encoder).reshape((-1,1)),\n",
    "    1.0 - (preproc_target(val_df, target_encoder).reshape((-1,1))),],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2f674-99be-48ec-a61e-5fb604dc6444",
   "metadata": {},
   "source": [
    "### Load in trained model\n",
    "We will now get a model from our most recent run, which will use to do inference and calculate metrics. In this example we are using the ML Flow Models module to load in the model which was saved as part of our training run. Saving and loading models through ML Flow models provides a consistent interface for interacting with ML models even if the you change the underlying framework used to train the model. In this notebook we will explore different ways of selecting and loading models, including:\n",
    "* getting a model from a specific run of a specific experiment\n",
    "* getting the latest run from a specific experiment\n",
    "* searching for the best model from a specific experiment based on a specified metric\n",
    "* getting a model by name from the model registry\n",
    "\n",
    "\n",
    "(You will need to have the ML Flow server we set uop and started in notebook 3 Model Development to run this code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5634e-423f-4098-972b-9d22af0b0d3b",
   "metadata": {},
   "source": [
    "First set up our access to the ML Flow server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f46c4-e8b2-4d59-b3bb-147e785fb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdee2a1-0ca0-4ff8-9171-2ca40c65c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_server_address = '127.0.0.1'\n",
    "mlflow_server_port = 5001\n",
    "mlflow_server_uri = f'http://{mlflow_server_address}:{mlflow_server_port:d}'\n",
    "mlflow_server_uri    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4f424-998e-439e-88eb-6740238f4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(mlflow_server_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f17eed-40c7-4de2-9990-3bf1240bce15",
   "metadata": {},
   "source": [
    "### get the latest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5fa98-4f92-4df7-97d0-1736d5c953fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_rotors_experiment_name = 'rse_mlops_demo_rotors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e35c21-6209-429a-816b-c7c47dc39417",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_rotors_experiment = mlflow.get_experiment_by_name(rse_rotors_experiment_name)\n",
    "rse_rotors_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2240f6f-1ff4-40ff-a891-9b04842e9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run_info  = sorted(mlflow.list_run_infos(rse_rotors_experiment.experiment_id), key=lambda run1: run1.end_time)[-1]\n",
    "latest_run_info.run_id, datetime.datetime.fromtimestamp(latest_run_info.end_time / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251ccf7-0990-4bc4-905f-843e7dfde870",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_rotors_model = mlflow.keras.load_model(latest_run_info.artifact_uri + '/model')\n",
    "rse_rotors_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c5050-a8af-4f28-849e-ce9a94437c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rse_rotors_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223ddb7-db5a-4cf5-bff2-37f95bca856f",
   "metadata": {},
   "source": [
    "Alternatively we can access the model through the URI specififed as `runs:/{run_id}` as described in the [ML Flow concepts page](https://www.mlflow.org/docs/latest/concepts.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dcdbe-6271-4ef4-9b3b-13e15056ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run_saved_model_uri = f'runs:/{latest_run_info.run_id}/model'\n",
    "latest_run_saved_model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbe4bc-37ad-4767-868a-802665459cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.keras.load_model(latest_run_saved_model_uri).predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469940f-8b11-4693-9797-c74c72d7a4f2",
   "metadata": {},
   "source": [
    "### Using the MLFlow model wrapping interface\n",
    "\n",
    "You can use the ML Flow models module to be able to use models from different frameworks interchangeably for inference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7958c23-2389-4c16-8151-f7533bd1db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_model = 'runs:/c5becc89c46e4cfd82441cc37ab6afbf/model'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554f5d2-2926-49e3-9cbb-fcf6e5bf74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e86f8-77e9-475a-a068-e31d107fcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(loaded_model.predict(X_val).argmax(axis=1) == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4842a3-2849-43a2-8190-0c2be61dd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d469a2-2872-4b5e-abc4-fcdee0adfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_val[:,0]), sum(y_val[:,1]), y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90c865-84d7-462b-b990-bb8be0375449",
   "metadata": {},
   "source": [
    "### Working with the ML Flow Client object and ML Flow UI\n",
    "\n",
    "The `MlflowClient` class gives us more general and low level access to the artifacts stored by the ML Flow server. Here we will use it to access a particular regiostered model and also to serch for the best model from all the runs in an experiment. A registered model is one from a particular run that we have decided to be of some significance. Usually at some point in a project you might decide that a particular model is best for some reason (usually perfrmance according to the chosen metric for the project). A model registry is a bit like for model what a catalogue is for data. One could image a research group maintaining a collective model registry to go with its colective data catalogue for commly used datasets and code rpository for common code.,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa9cb1-e916-49c1-8f43-64b8d266d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client = mlflow.tracking.MlflowClient(tracking_uri=mlflow_server_uri)\n",
    "mlflow_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa7e6a-5044-4dd4-8eec-a98ecce57b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(m1.name, m1.version) for m1 in mlflow_client.list_registered_models()[0].latest_versions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f8313-b1e3-4bd5-9c9b-21f1003f5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run_info.artifact_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fd33e-646e-47e4-842e-d50deb5b606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.tracking.MlflowClient().get_run(latest_run_info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db707db7-b86d-400a-98e3-1f6d3dd299c9",
   "metadata": {},
   "source": [
    "### Searching for the best model\n",
    "\n",
    "Once you've done various training runs, perhaps varying the architecture, hyperparameters, input data or just using different random initialisations, you will have multiple runs in your experient (and also possiblly multiple experiments). You may then want to retrieve the model that is the best or highest scoring according some metric of interest, which you may do multiple times for different metrics of interest.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef79acb-b157-4356-8305-02f6228e8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client.list_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3942aaa-da2c-4820-a146-209009327cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client.list_experiments()[-1].experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929f131-ec6b-4983-bb1f-e05be3358ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_client.search_runs(mlflow_client.list_experiments()[-1].experiment_id, order_by=['metrics.val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e7a36-39d6-4cb5-9f9d-fd8d559de377",
   "metadata": {},
   "source": [
    "### Loading a registered model\n",
    "\n",
    "When you have a model that is performing well or is in some other way of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8752b2-0b57-42df-b743-b50cb0b32085",
   "metadata": {},
   "source": [
    "As a dirst option you can use the API to find the registered models and load one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e4356-c66b-4295-b816-87587996f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlflow_client.list_registered_models()), [(m1.name, m1.latest_versions[-1].version) for m1 in mlflow_client.list_registered_models()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455a91d-755a-4057-a173-69aa69246e1f",
   "metadata": {},
   "source": [
    "Now we can load the model through the information in the registered model item of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64287a-38b7-4b91-ab63-a6dd78305dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotors_reg_model_name = 'rse_rotors_20220819'\n",
    "rotors_reg_model_version = '1'\n",
    "reg_model_artifact_uri = f'models:/{rotors_reg_model_name}/{rotors_reg_model_version}'\n",
    "reg_model_artifact_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f402d1-cc1a-4764-b0d9-c24de4a568e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = mlflow.pyfunc.load_model(mlflow_client.get_registered_model(rotors_reg_model_name).latest_versions[0].source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c01579-b9dd-483d-a242-2c1906fbea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceff222-4767-4e13-9b1e-e0a0bb1f1977",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "Now that we have access to one or more trained models, typically we will want to calculate performance and explainability metrics with the models. For this example of our wind rotors models, we are going to calculate some [standard metrics for a classification problem](https://en.wikipedia.org/wiki/Precision_and_recall), such as \n",
    "* precision\n",
    "* recall\n",
    "* F1-score\n",
    "* false alarm rate\n",
    "* miss rate\n",
    "\n",
    "\n",
    "We will also calculate a *domain specific* metric called [*Symmetric Exteremal Dependence Index*](https://www.ecmwf.int/en/newsletter/147/meteorology/use-high-density-observations-precipitation-verification#:~:text=The%20Symmetric%20Extremal%20Dependence%20Index,a%20threshold%20to%20be%20set), which is a metrics specific to weather and climate probklems, rather than a generic machine learning metric. Most research ML research problems should consider both standard ML metrics, as well the measures of algorithm performance that are specific to the problem domain e.g. meteorlogy, genetics, astrophysics, medicine etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e68b3-35d6-4a1a-bfa4-d4eacaade949",
   "metadata": {},
   "source": [
    "As we can see from the output in the cells above, our model outputs a pseudo probability of a rotor being observed in a given time window, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16172e0-125b-4e63-bc31-6cff2ef05f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sedi(conf_mat):\n",
    "    \"\"\"\n",
    "    Define a function to calculate the Symmetic Extermal Dependence Index (SEDI) from a confusion matrix\n",
    "    \"\"\"\n",
    "    hr1 = conf_mat[1,1] / (conf_mat[1,0] + conf_mat[1,1])\n",
    "    fa1 = conf_mat[0,1] / (conf_mat[0,0] + conf_mat[0,1])\n",
    "    sedi_score1 = (\n",
    "        (numpy.log(fa1) - numpy.log(hr1) - numpy.log(1.0-fa1) + numpy.log(1.0-hr1) )\n",
    "        / (numpy.log(fa1) + numpy.log(hr1) + numpy.log(1.0 - fa1) + numpy.log(1.0-hr1) )  )\n",
    "    return sedi_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b3513-1be3-4dbe-a90b-badb4ebaf5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result_val = rse_rotors_model.predict(X_val)[:,0] >2e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f6249-99f7-430c-82f1-05aedaf6ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = matplotlib.pyplot.figure('comparison of class frequencies for different thresholds', figsize=(18,6))\n",
    "ax1 = fig1.add_subplot(1,3,1)\n",
    "pandas.Series(rse_rotors_model.predict(X_val)[:,0] >2e-3 ).value_counts().plot.pie(legend=True, autopct='%.2f',labels=['No Rotors', 'Rotors Present'], ax=ax1)\n",
    "ax1 = fig1.add_subplot(1,3,2)\n",
    "pandas.Series(rse_rotors_model.predict(X_val)[:,0] >2e-2 ).value_counts().plot.pie(legend=True, autopct='%.2f',labels=['No Rotors', 'Rotors Present'], ax=ax1)\n",
    "ax1 = fig1.add_subplot(1,3,3)\n",
    "pandas.Series(rse_rotors_model.predict(X_val)[:,0] >2e-1 ).value_counts().plot.pie(legend=True, autopct='%.2f',labels=['No Rotors', 'Rotors Present'], ax=ax1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d427fd-55d9-4081-af46-701e8ef2131a",
   "metadata": {},
   "source": [
    "For this model, we can choose different threshold to classify the result as *no rotors* vs *rotors present*. With different choices of threshold, we will get different metric results. We can choose the threshold that gives us the best performance for our particular requirements for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa69920-e315-4e06-bd27-239ba4626e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_raw = rse_rotors_model.predict(X_train)[:,0]\n",
    "y_val_pred_raw = rse_rotors_model.predict(X_val)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0c08b-7b6a-42d5-8e63-90aed00d2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_list = list(numpy.arange(1e-3,0.3,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cfdd0-560c-448b-8e82-216d01f94560",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rates = []\n",
    "hit_rates_test = []\n",
    "false_alarm_rates = []\n",
    "false_alarm_rates_test = []\n",
    "sedi_list = []\n",
    "sedi_list_test = []\n",
    "for threshold in numpy.arange(1e-3,0.3,1e-3):\n",
    "    y_pred_train = list(map(float, y_train_pred_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_train[:,0], y_pred_train)\n",
    "    hit_rates += [cm1[1,1] / (cm1[1,0] + cm1[1,1])]\n",
    "    false_alarm_rates += [cm1[0,1] / (cm1[0,0] + cm1[0,1])]\n",
    "    sedi_list += [calc_sedi(cm1)]\n",
    "\n",
    "    y_pred_val = list(map(float, y_val_pred_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_val[:,0], y_pred_val )\n",
    "    hit_rates_test += [cm1[1,1] / (cm1[1,0] + cm1[1,1])]\n",
    "    false_alarm_rates_test += [cm1[0,1] / (cm1[0,0] + cm1[0,1])]\n",
    "    sedi_list_test += [calc_sedi(cm1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e8635-8934-4984-9635-3213ed70054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = matplotlib.pyplot.figure(figsize=(16,10))\n",
    "ax1 = f1.add_subplot(1,2,1,title='hit rates vs false alarms vs SEDI (training data)')\n",
    "ax1.plot(thresholds_list, hit_rates,'r')\n",
    "ax1.plot(thresholds_list, false_alarm_rates,'b')\n",
    "ax1.plot(thresholds_list, sedi_list,'k')\n",
    "\n",
    "ax1 = f1.add_subplot(1,2,2,title='hit rates vs false alarms vs SEDI (test data)')\n",
    "ax1.plot(thresholds_list, hit_rates_test,'r')\n",
    "ax1.plot(thresholds_list, false_alarm_rates_test,'b')\n",
    "ax1.plot(thresholds_list, sedi_list_test,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70f193-2d08-42b7-be5e-96449d9cb3a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example - Dashboard to visualise results\n",
    "\n",
    "In any research project we want to share the results with others working on the project, in our research group, current and future collaborators and other researchers around the world in the same field. To do that, we don't just want to be able plot the results as we did above for ourselves, which we amy do for creating reports and papers, but ideally make it possible for other to interactively expore the results through a *results dashboard*. Here will use the same tool (panel from the holviz ecosystem) that we used in the notebook 2 to create a dashboard for data exploration to creaste a dashboard to enable researchers to explore the results of the trained machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c9dd6-a158-4b35-94a3-5e18f5a63ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82a8ca-0313-4de7-9d4c-44a0483ee6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e6a46-40c5-4647-bc67-2ff7363f5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rotors_metrics(rotors_model,  X_dict, y_dict, selected_split, threshold,):\n",
    "    y_raw = rotors_model.predict(X_dict[selected_split])[:,0]\n",
    "    y_pred = list(map(float, y_raw > threshold)) \n",
    "    cm1 = sklearn.metrics.confusion_matrix(y_dict[selected_split], y_pred)\n",
    "    metric_dict = {\n",
    "        'hit_rate' : [cm1[1,1] / (cm1[1,0] + cm1[1,1])],\n",
    "        'false_alarm_rate' : [cm1[0,1] / (cm1[0,0] + cm1[0,1])],\n",
    "        'sedi_score': [calc_sedi(cm1)],\n",
    "    }\n",
    "    \n",
    "    fig1 = matplotlib.pyplot.Figure()\n",
    "    ax1 = fig1.add_subplot(1,1,1)\n",
    "    pandas.DataFrame.from_dict(metric_dict).transpose().plot.bar(ax=ax1)\n",
    "    ax1.set_ylim([0.0, 1.0])\n",
    "    fig1.tight_layout()\n",
    "    return fig1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b84ab9-8c1e-4698-93ce-510f03b3946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rotors_metrics(\n",
    "    rotors_model=rse_rotors_model,\n",
    "    X_dict= {'train': X_train, 'val': X_val},\n",
    "    y_dict= {'train': y_train[:,0], 'val': y_val[:,0]},\n",
    "    selected_split='val',\n",
    "    threshold=0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3f9dc-ee61-4309-a614-e01821f02b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_select_widget = panel.widgets.Select(options=['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b0a9f-2926-46cd-979b-5870346d66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_widget = panel.widgets.slider.FloatSlider(start=0.0,end=0.2,step=0.001, value=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b457e5-0cb6-4c01-845a-ce5eca505912",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_plotter = panel.bind(\n",
    "    functools.partial(plot_rotors_metrics, \n",
    "                      rotors_model=rse_rotors_model,\n",
    "                      X_dict= {'train': X_train, 'val': X_val},\n",
    "                      y_dict= {'train': y_train[:,0], 'val': y_val[:,0]},),\n",
    "    selected_split=split_select_widget,\n",
    "    threshold=threshold_widget,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac6d68-03a4-4aab-8ade-52920a88bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dashboard = panel.Column(panel.Row(split_select_widget, threshold_widget), metric_plotter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefa54f-2c91-478d-a02a-c5edf689ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dashboard.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd99e5a-97ed-4678-8708-59986c829576",
   "metadata": {},
   "source": [
    "Our very basic dashboard is:\n",
    "* loading data from a catalog\n",
    "* loading a pre-trained model from catalog of pretrained models\n",
    "* display plots  created on demand through an interactive dashboard.\n",
    "\n",
    "Although this seems very simple (well, it is), this is a very scalable approach. We can easily make this better by\n",
    "* Using a larger dataset.\n",
    "* scaling up the calculation required through a workflow engine (e..g. dask, ray)\n",
    "* run this on the cloud\n",
    "* use more complicated/interesting plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a922e-1b0f-42a2-af94-098dcca0903b",
   "metadata": {},
   "source": [
    "# Example - Explainable AI\n",
    "\n",
    "Lastly we'll briefly touch on an important emerging part of machine learning called interpretability or explainability or explainable AI (XAI). This aims to open up the \"black box\" of the machine learning algorithms and try to understand how predictions are made and what they mean. Often this reolves in understanding the contribution or importance of a particular feature or data point in the training set. In many ways this is calculated and displayed and stored in the same way as the performance metrics displayed above, so from a workflow or engineering perspective it is perhaps not that noteworthy. It is an increasingly important and impactful part of a machine learning pipeline and researchers are generally now expected to include some aspect of XAI in any publications for research depending on machine learning, so it is important for RSEs to be aware of this important class of tools which they will likely have to support in ML projects, including seamless scaling up on XAI computation and being able to do these calculations in a reliable, reproducible way. \n",
    "\n",
    "Common tools\n",
    "* [scikit learn permutation featue importance](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
    "* [SHAP](https://shap.readthedocs.io/en/latest/index.html)\n",
    "* [OmniXAI](https://github.com/salesforce/OmniXAI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1dcd2-80ca-486f-8c5e-2e4545063d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1f7e2-f8fd-4792-909b-c8485550bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1877f-a1df-4cc5-81d8-a6c7198e330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_baseline = sklearn.metrics.recall_score(y_val[:,0] , reg_model.predict(X_val)[:,0] > 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca985f81-f312-4dad-862d-ed36b73ccc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pfi_metric(rotors_reg_model_name, X_val, y_val, ix1, ix2):\n",
    "    reg_model = mlflow.pyfunc.load_model(mlflow_client.get_registered_model(rotors_reg_model_name).latest_versions[0].source)\n",
    "    pfi_X = copy.deepcopy(X_val)\n",
    "    pfi_X[:,ix1] = X_val[:,ix2]\n",
    "    pfi_X[:,ix2] = X_val[:,ix1]\n",
    "    recall_accum = sklearn.metrics.recall_score(y_val[:,0] , reg_model.predict(pfi_X)[:,0] > 1e-2)\n",
    "    return recall_accum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91ffef-120a-4690-bdf7-59b498766306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74214c46-c4f6-4249-88f1-4bc1ca1d7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51317ba1-2cbd-44e2-bad2-87957a98950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "recall_average_changes = numpy.zeros(X_val.shape[1])\n",
    "for ix1 in range(X_val.shape[1]):\n",
    "    print(ix1)\n",
    "    recall_diffs = sum([calc_pfi_metric(rotors_reg_model_name, \n",
    "                                        X_val, \n",
    "                                        y_val, \n",
    "                                        ix1, \n",
    "                                        random.randint(0,87)) - recall_baseline \n",
    "                        for i2 in range(3)])\n",
    "    recall_average_changes[ix1] =  recall_diffs / 3.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e246280-a794-4f67-a996-f898a83fa4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(recall_average_changes).plot.bar(figsize=(16,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23697e-c0b7-45c2-8cba-fb73e38e24ea",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [Explainable AI - Towards Data Science](https://towardsdatascience.com/what-is-explainable-ai-xai-afc56938d513)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9d47a-68f5-41d4-b779-080a63ede546",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "* [Holoviz](https://holoviz.org/) \n",
    "* [ML Flow Models](https://www.mlflow.org/docs/latest/models.html)\n",
    "* [scikit learn model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "* [OmniXAI](https://github.com/salesforce/OmniXAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
